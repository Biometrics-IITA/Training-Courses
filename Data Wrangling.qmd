---
title: "Untitled"
format: html
editor: visual
---

---
title: "<p style=\"color:black,text-align:center\"> Data Wrangling</p>"
author: 
  - name: <font color=#ff6600><b>Biometrics Unit</b></font> 
    affiliation: <font color=#ff6600><b>International Institute of Tropical Agriculture (IITA)</b></font>
format:
  html:
    html-math-method: mathjax
    css: "style.css"
    toc: true
    toc-location: right
    toc-title: Outline
    toc_float:
    collapsed: True
    smooth_scroll: true
    code_folding: show
    number_sections: false
    theme: readable
    highlight: monochrome
    fig_width: 5
    fig_height: 5
    fig_caption: true
    df_print: paged
    xaringan::moon_reader: null
    lib_dir: libs
    nature:
      highlightStyle: github
---

```{r setup, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
#options(max.print="80")
opts_chunk$set(tidy=TRUE, message=FALSE, warning=FALSE)
#opts_knit$set(width=80)
```

# [**Introduction**]{style="color: #2C6D26;"}

Data wrangling is the process of transforming raw data into a more organized and structured format, which facilitates improved insights and better decision-making. Imagine your data as a set of puzzle you need to solve, data wrangling is a tool that will significantly help you to organise the data, making it much easier to solve the data puzzles.

There are six stages involved in data wrangling, they are highlighted below:

1.  **Data Discovery:** Data discovery is the *process of uncovering and exploring valuable insights* within data. This usually involves collecting data from different sources, transforming and merging it, and employing visualization and analytical methods to reveal patterns, trends, and insights. The aim is to make data more accessible and actionable for decision-making, enabling users to grasp complex information and address specific business questions.

    Data discovery is the process that enables users to visually explore data and apply advanced analytics to uncover patterns, gain insights, answer specific questions, and extract value from business information. It involves integrating and transforming data from multiple sources, analyzing data structures, and using visualization techniques to gain insights and extract valuable information.

2.  **Data Structuring:** Data structures are specialized formats for organizing data on a computer to ensure efficient processing, storage, and retrieval. They provide a systematic way to manage information, making it easier to access and use. A data structure is a *method for organizing and managing data*. It helps in gathering different types of data, whether structured or unstructured, and transforming it into useful and meaningful information. An array is an example of data structure.

3.  **Data Cleaning:** Data cleaning is a vital stage in the data science workflow, focusing on *detecting and rectifying errors, inconsistencies, and inaccuracies in the data to enhance its quality and usability*. This process is crucial because raw data often contains noise, gaps, and inconsistencies that can adversely affect the *accuracy and dependability* of the insights generated from it. Data cleaning involves preparing data for analysis by *correcting or eliminating data that is incorrect, incomplete, irrelevant, duplicated, or poorly formatted*. It involves steps such as *removing unwanted observations, managing structure errors, managing unwanted outliers and handling missing data*.

4.  **Data Enriching:** After transforming your data into a more usable format, evaluate whether you have all the necessary information for your analysis. If not, you can enhance it by incorporating values from additional datasets. This is also a good time to consider adding metadata to your database.

5.  **Data validation:** Once you’ve converted your data into a more usable format, assess if you have all the information required for your analysis. If anything is missing, you can augment it by integrating data from other sources. Additionally, this is an opportune moment to add metadata to your database. Validation *guarantees the quality and reliability of your processed data.* It involves checking for inconsistencies, verifying data integrity, and ensuring the data meets predefined standards. This process helps build confidence in the dataset's accuracy and ensures it is suitable for meaningful analysis.

6.  **Data Publishing:** Data publishing refers to the process of making data available to users, often by sharing or disseminating it through various platforms or channels. This can involve publishing datasets on websites, data repositories, or databases, *ensuring that the data is accessible, usable, and properly documented for others to analyze or utilize.*

    Also, there are three main aspect of data wrangling. These includes:

    1.  Tibbles
    2.  Data import
    3.  Tidy data

## [Tibbles]{style="color: #002D62;"}

Tibbles are a key feature of the tidyverse, distinguishing it from most other R packages, which typically use standard data frames. You can convert a dataset to tibbles with `as_tibble()`

**Example**

```{r}
library(tidyverse) #load tidyverse package
library(agridat)  #load agridat package
dat <- (australia.soybean)
dat <-  as_tibble(dat)  #convert dataframe to tibble
dat 
```

<br><br>

You can create tibbles from raw data using `tibble()` as shown below

**Example**

```{r}
tibble (
  x = 1:10,
  y = rep(1:5,2),
  z = x + y
)
```

<br><br>

**What are the common differences between a tibble and a dataframe**

A tibble is considered a neater format of a data frame and its often used in tidyverse and ggplot2 packages. Tibble has an advanced print function and only shows the first ten rows with all the columns fitted on the screen. The data type is written just below the heading of each column. This does not apply to data frame. Tibble can be used for indexing such as \$, \[\[ \]\]. \$ extracts using name while \[\[ \]\] extract by name and position.

**Example**

```{r}
set.seed(234)
df <- tibble(
  x = runif(5),
  y = rnorm(5)
)

# Extract by name
df$x

df[["x"]]

# Extract by position
df[[1]]

```

<br><br>

## [Data Import]{style="color: #002D62;"}

**Importing a Comma Seperated Version (CSV)**

Data import can be done with the `readr` package which is a core `tidyverse` package. This is used for reading data stored in text file spreadsheets into R. Some readr's function are used in turning flat files into dataframe. You can load readr using the code `library(readr)`, this gives you access to functions such as: `read_csv()` for reading comma delimited files, `read_csv2()` for semicolon separated files, `read_table` for white space separated values etc.

```{r}
#install readr package
install.packages("readr")

# load the package
library(readr)

#read the csv file into a tibble

data <- read_csv("steptoe.morex.pheno.csv")
data
```

<br><br>

**Importing an Excel Version (XLSL)**

The readxl library can also be assessed from the readr package and it is used to import excel files. The functions to import excel files are `read_excel()` or `read_xlsx()`. The `read_excel()` auto detect the format while `read_xlsl()`permits more than two sheets in a file.

**Example**

```{r}
#install the readxl package
#install.packages("readxl") 

#load the readxl package
library(readxl)

#Read the Excel file into a tibble
dat <-  read_excel("Sugar cane.xlsx", sheet = "Sugar cane")
dat

```

<br><br>

## [Tidy Data]{style="color: #002D62;"}

Tidy data considers ways to convert your messy dataset into format that are clean and can be easily analysed. The aim of tidyr is to assist in creating tidy data, where:

-   Each variable is represented as a column, and each column corresponds to a variable.
-   Each observation is represented as a row, and each row corresponds to an observation.
-   Each value is contained within a cell, with each cell holding a single value.

The principles of tidy data offer a standardized method for organizing data values within a dataset. The tidy data standard is designed to ease the initial exploration and analysis of data and to streamline the creation of data analysis tools that integrate seamlessly.

Various functions can be used in the tidyr package, functions such as *pivoting (longer and wider), rectangling, nesting, splitting, replace and drop na* etc.

# [**Examples**]{style="color: #2C6D26;"}

## [Data Frame]{style="color: #002D62;"}

**Let's create a data frame**

```{r}
trial01 <- data.frame(
  variety = c("G01-US234", "G05-BT456", "Ind01","G11-DR234", "Check"), 
  yield = c(6323.3, 2515.2, 5611, 7729, 7843.25),
  height = c(123.30, 95.2, 113, 89.45, 145.67)
  )
```

**Let's display the data frame**

```{r}
trial01  ## display the object in Q2
View(trial01) ## display the object in Q1

#trial01[R,C]
trial01[1:5,]
```

<br><br>

**We can extract the first three rows:**

```{r}
# object[1:nrows, 1:ncolumns]
trial01[1:5, 1:3]
trial01[1:3, ] # the three first rows and all columns
```

```{r}
# We can extract the first two columns
trial01[, 1:2]

```

```{r}
# We can extract "from 3rd to 5th row" with "2nd and 3rd column"
trial01[3:5, 2:3]

trial01[,c(1,3)]
```

```{r}
# We can list the column names using any of this methods
names(trial01)

colnames(trial01)
```

```{r}
# We can extract specific column from a data frame using column name trial01$

yield1 <- trial01$yield

trial01$yield


```

```{r}
# trial01$Yield. R is case sensitive, yield is different from Yield

# We can find the mean of the extracted column using any of the codes below

mean(trial01$yield)
mean(yield1)

mean(trial01$yield)

```

```{r}
# We can add a column vector using a new column name
trial01$flowering <- c(87, 101, 88, 120, 90)
trial01$flowering <- c(87,101,88,120,90)
trial01

flowering2 <- c(87,101,88,120,90)

trial01$flowerin2 <- flowering2

trial01
```

<br><br>

## [The tidyverse package]{style="color: #002D62;"}

```{r}

library(tidyverse)
trial01

# Let's look at the structure of trial01

str(trial01) ## structure of trial01: what's trial01?
```

```{r}
# we can convert trial01 to a tibble and save the new created object into trial01.tibble
trial01.new <- as_tibble(trial01) 
trial01.new
# dbl(double) and int(integer) are numeric
```

## [**Data Import**]{style="color: #002D62;"}

```{r}
# Read csv file: supply the path to a file and you get the data into R

library(readr)
mydata <- read_csv("C:/Users/DOjekere/CGIAR/Fowobaje, Kayode Rapheal (IITA) - Training Materials/2024/R/RMD Training/Example-02.csv")
mydata


# If a project/working directory is created and we are working within the project,

mydata1 <- read_csv("Example-02.csv") # Tab keyboard
```

```{r}

## Read xlsx file
library(readxl)
iwu <- read_excel("C:/Users/DOjekere/CGIAR/Fowobaje, Kayode Rapheal (IITA) - Training Materials/2024/R/RMD Training/Example-03.xlsx", sheet = "whitecorn")

## Read Excel file:
mydata2 <- read_excel("Example-03.xlsx", sheet = "whitecorn")
View(mydata2)
mydata2
```

## [Data Transformation]{style="color: #002D62;"}

```{r}
library(tidyverse)

example02 <- read_csv("Example-02.csv")
example02
names(example02)
str(example02) ## really don't need when you have tibble

# I want to display the number of years
head(example02$year)
unique(example02$year)
unique(example02$loc) # having the number of locations 
```

<br><br>

**The pipe \|\>**

Pipes are a powerful tool for clearly expressing a sequence of multiple operations. Object \|\> (object is usually a tibble, a data) Function(argument1, argument2, ...)

### **Filter**

We can filter the data for 1970

```{r}

example02.70 <- example02 |> 
  filter(year == 1970)
example02.70
```

Filtering using the pipe is equivalent to below but not nice

```{r}
example02.70.not.nice <- example02[example02$year==1970,]

example02.70.not.nice

```

We can filter the data for one location :

```{r}
example02.Lawes <- example02 |> 
  filter(loc == "Lawes")

example02.Lawes

```

We can filter the data with multiple criteria (several arguments)

```{r}
example02.Lawes3.2 <- example02 |> 
  filter(yield > 3.2, loc == "Lawes")

example02.Lawes3.2 <- example02 |> 
  filter(yield >3.2, loc == "Brookstead" | loc == "Lawes")

example02.Lawes3.2

```

What does the following command do?

```{r}
example02 |> 
  filter(gen == "G01" | gen == "G02")

```

### **Arrange**

We can arrange example02 by yield in ascending order

```{r}
library(tidyverse)
library(readxl)
example02 <- read_csv("Example-02.csv")

example02 |> 
  arrange(yield)
```

We can arrange example02 by yield in descending order

```{r}
example02 |> 
  arrange(desc(yield))
```

We can arrange example02 by year, loc, gen

```{r}
example02 |> 
  arrange(year, loc, gen)

example02 |> 
  arrange(desc(year), loc, gen)

```

### **Select**

When working with many variables, it can be a good practice to narrow the dataset and consider only few variables for analysis. Let's only consider the location, year, genotype, yield, and height

```{r}
example02.short <- example02 |> 
  select(loc, year, gen, yield, height)

example02.short
```

If we are interested in moving a particular variable to the first column in the dataframe, select() and everything() can do that. We can also move more than one variable

```{r}
example02 |> 
  select(year, everything())

example02 |> 
  select(year, loc, gen, everything())
```

**NOTE:** Select refers to columns: select (yield, loc, gen) -- choose the columns arrange refers to rows: arrange(loc, desc(yield)) -- sort

```{r}
example02 |> 
  select (yield, loc, gen)

```

This code will not work because there is no variable named "desc(yield)"

```{r}
#example02 |>        
#  select (loc, gen, desc(yield))
```

But what we want is:

```         
1. select the three variables, AND
2. sort the yield in desc order
```

This can be done with:

```{r}
new <- example02 |> 
  select (loc, gen, yield) |> 
  arrange(desc(yield))
```

<br><br>

### **Add New Variable**

We can add new columns that are functions of existing columns with `mutate()` which always adds new columns at the end of the data new column

```{r}
new1 <- example02.short |> 
  mutate(yield_kg_ha = yield * 1000)
```

**Replace an existing column**

```{r}
new2 <- example02.short |> 
  mutate(yield = yield * 1000)
```

### **Summarize**

We can be interested having the mean of yield

```{r}
mean(example02$yield, na.rm = TRUE)

a <- c(2,3,4)
a
mean(a)
mean(a, na.rm = TRUE)

b <- c(2,3,4,NA)
b
mean(b)
mean(b, na.rm = TRUE)

summary(example02) # summary of the dataframe

```

**But we prefer using summarize from tidyverse**

`Summarize()` collapses a data frame to a single or few row(s)

```{r}
example02 |> 
  summarize(yield_mean = mean(yield, na.rm=TRUE))
```

**Summarize by group**

```{r}
unique(example02$loc)

example02 |>
  group_by(loc) |>
  summarise(yield_loc = mean(yield, na.rm = TRUE))

example02 |>
  group_by(loc, year) |>
  summarise(yield_loc = mean(yield, na.rm = TRUE))

example02 |>
  group_by(gen) |>
  summarise(yield = mean(yield, na.rm = TRUE)) |> 
  arrange(desc(yield))

example02 |>
  group_by(gen, loc) |>
  summarise(yield = mean(yield, na.rm = TRUE)) |> 
  arrange(desc(yield))
```

### **Missing values**

```{r}
missing_dat <- read_csv("Example-02-missing.csv")

mean(missing_dat$yield, na.rm = TRUE)
```

**How to get the missing data if any**

```{r}
example02.missing <- read_csv("Example-02-missing.csv")
missing_dat |> 
  group_by(loc) |> 
  summarize(Nmissing=sum(is.na(yield)))

```

Select the obs with a missing yield

```{r}
missing_dat |> 
  filter(is.na(yield))
```

The total number of obs is 464

```{r}
example02.missing |> 
  filter(!is.na(yield))

```

464 obs = 461 valid + 3 missing

```{r}
head(is.na(example02.missing$yield))
sum(is.na(example02.missing$yield))

```

### **Count**

Counting the number of observations

```{r}
example02 |>
  group_by(loc) |>
  summarise(new4 = n())
```

### **Factors**

The function as.factor() convert a variable to a factor

```{r}
example02$env <- as.factor(example02$env)
example02$loc <- as.factor(example02$loc)
example02$gen <- as.factor(example02$gen)

```

This is equivalent to the code below using mutate() and the pipe \|\>

```{r}
example02 <- example02 |>
  mutate(
    env=factor(env),
    loc=factor(loc),
    gen=factor(gen)
  )

example02 |>
  mutate(across(c(env, loc, gen)))
```

To display the levels of a factor

```{r}
levels(example02$loc)
```

To get the number of levels of a factor

```{r}
nlevels(example02$env)
```

<br><br>

# [**Pivoting**]{style="color: #2C6D26;"}

Data management is an important task during data analysis because most times, datasets do not come in the required shape for analysis and result presentation. Hence, the need for conversion of dataset tables from wide to long format, or vice versa. It is important to understand data frame intuition where variables are in columns, observations are in rows, and values are in the cell.

![](images/tibble.png)

The `pivot_wider()` or `pivot_longer()` functions in R **tidyverse** package help reorganize or reshape data values into the needed layout.

![](images/Pivot.jpg) <br><br>

## [**Pivot Longer**]{style="color:#002D62;"}

A common problem is a dataset where some of the column names are not names of variables, but values of a variable. The function `pivot_longer()` transform the dataset in wide format to longer.

Below is the basic R syntax needed to transform data from a wide format to a long format:

```{r eval = FALSE}

pivot_longer(data, cols, names_to = "xxx", values_to = "yyy") #<<
```

-   `data`: a data frame to pivot longer
-   `cols`: columns to pivot into long format
-   `names_to`: a character specifying the name of the selected columns to pivot
-   `values_to`: a character specifying the name of the cell values in the pivoted columns.

## [**Example**]{style="color: #002D62;"}

To illustrate how this function works, let’s import the Australia Soybean 1970 dataset.

```{r echo=TRUE, results="tiny"}
#library(tidyverse)
dat <- read_csv("australia.soybean1970.csv")

```

We have a data frame with 7 variables (env, loc, year, gen, yield, height, and lodging) and 232 observations. The data frame is stored under an object named **dat**

```{r echo=TRUE, results="tiny"}
dat
```

As you can see, the dataset is already in wide format as seen from the previous slide. To transform the dataset into a long format, we use the function `pivot_longer`

```{r echo=TRUE, tidy=FALSE}
dat_longer <- dat |>
  pivot_longer(yield:lodging, names_to = "trait", values_to = "values")

```

```{r echo=TRUE, results="tiny"}
dat_longer
```

<br><br>

## [**Pivot Wider**]{style="color: #2C6D26;"}

The function `pivot_wider()` is the opposite of the `pivot_longer()` function. It transforms the dataset from long format to wider. Below is the basic R syntax needed to transform data from a wide format to a long format:

```{r eval = FALSE}
pivot_wider(data, names_from = "xxx", values_from = "yyy") #<<
```

-   `data`: a data frame to pivot wider
-   `names_from`: the name of the output column
-   `values_from`: the name of the column to get the cell values from

## [**Example**]{style="color: #002D62;"}

To transform the dataset into a wide format, we use the function `pivot_wider()`

```{r echo=TRUE, results="tiny"}
library(tidyverse)
dat <- read_csv("Longer.csv")

```

```{r echo=TRUE, results="tiny"}
dat
```

```{r echo=TRUE, tidy=FALSE}
dat_wider <- dat |>
  pivot_wider(names_from = "trait", values_from = "values")
```

```{r echo=TRUE, results="tiny"}
dat_wider
```

<br><br>

# [**Combining Tables**]{style="color: #2C6D26;"}

So far we have seen how tidy data can be reshaped. However, there are occasions when dataset comes from different data sources and we need to combine them together before we proceed further to analysis or result presentation. The `dplyr` in R **tidyverse** package offers different mutating join functions to combine datasets together. A mutating join function allows you to combine variables from two tables by matching observations by key variable(s), then copy variables from one table to another.

There are four common mutating joins: `left_join`, `right_join`, `inner_join`, and `full_join`. They all have the same arguments but return different tables, *df1* and *df2* are a pair of data frames to join, and *by* is a unique character vector variable(s) to join the data frames by.

## [**Left Join**]{style="color: #002D62;"}

`left_join(df1, df2, by)`

```{r}
#join matching values from df2 to df1 
```

<img src="images/left1.jpg" width="30%"/>

<img src="images/left2.jpg" width="30%"/>

A left join in R is a kind of database join that joins two data frames together by their columns using a shared key. Only the matching rows from the right data frame are included in the result of a left join, which contains all of the rows from the left data frame. NA values are entered for each column in the corresponding data frame if there is no match.\
\
**Example** To demonstrate how these mutating joins work, let’s import the pair of data frames needed.

```{r echo=TRUE}
library(readxl)
dat1 <- read_excel("df.combine.xlsx", sheet="df1")
dat2 <- read_excel("df.combine.xlsx", sheet="df2")
```

**dat1** has 15 observations with five variables:gen, yield, height, size, and protein.

**dat2** has 15 observations with two variables:gen, oil.

Display the observations in dat1

```{r echo=TRUE, results="tiny"}
dat1
```

Display the observations in dat2

```{r echo=TRUE, results="tiny"}
dat2
```

```{r echo=TRUE, results="tiny"}
# Left join
df_left <- left_join(dat1, dat2)  
df_left
```

-   The code performs a left join on `dat1` and `dat2`, resulting in a new data frame `df_left` that contains all rows from the left data frame (`dat1`) along with matching rows from `dat2`. Rows in `dat1` without a corresponding match in `dat2` will have `NA` in their resulting columns for data from `dat2`.

    <br><br>

## [**Right Join**]{style="color: #002D62;"}

`right_join(df1, df2, by)`

```{r}
#join matching values from df2 to df1 
```

<img src="images/right1.jpg" width="30%"/>

<img src="images/right2.jpg" width="30%"/>

A right join is a type of database join that combines two data frames based on a common key, similar to a left join but with a focus on retaining all the rows from the right data frame. When performing a right join, all rows from the right data frame are included in the result, and only the matching rows from the left data frame are included. If there is no match, the corresponding columns of the left data frame will be filled with `NA` values

**Example**

Continuing with the previous data,

```{r echo=TRUE, results="tiny"}
# Right join
df_right <- right_join(dat1, dat2)  
df_right
```

The right join operation retains all rows from the right data frame (`dat2`) and only those rows from the left data frame (`dat1`) that match based on the specified keys. If there are entries in `dat2` without corresponding matches in `dat1`, the output will show `NA` in place of those unmatched entries from `dat1`

<br><br>

## [**Inner Join**]{style="color: #002D62;"}

`inner_join(df1, df2, by)`

```{r}
#join values from df2 to df1 and retain only matching rows
```

<img src="images/inner1.jpg" width="30%"/>

<img src="images/inner2.jpg" width="30%"/>

An inner join is a type of database join that combines two data frames (or tables) based on a common key or keys, returning only those rows where there is a match in both data frames. In other words, an inner join retrieves the intersection of the two dataset

**Example**

```{r echo=TRUE, results="tiny"}
# Inner join
df_inner <- inner_join(dat1, dat2)  
df_inner
```

The resulting data frame (`df_inner`) contains only those rows where there are matching values in the `id` column of both `dat1` and `dat2`

<br><br>

## [**Full join**]{style="color: #002D62;"}

`full_join(df1, df2, by)`

```{r}
#join values from df2 to df1 and retain all rows, all values 
```

<img src="images/full1.jpg" width="30%"/>

<img src="images/full2.jpg" width="30%"/>

A full join (or full outer join) is a type of database join that combines the results of both left and right joins. This means that a full join returns all the rows from both data frames (or tables), regardless of whether there is a match between them. If a row in one data frame does not have a corresponding match in the other, the result will contain `NA` (or missing values) for the columns of the unmatched row from the other data frame.

**Example**

```{r echo=TRUE, results="tiny"}
# Full join
df_full <- full_join(dat1, dat2)  
df_full
```

The code `df_full <- full_join(dat1, dat2)` effectively creates a new data frame that combines data from `dat1` and `dat2`, ensuring that all records are retained, either matched or unmatched. This type of join is particularly useful in exploratory data analysis and situations where preserving all relevant data is critical for further analysis and interpretation.

<br><br>

# [**Creating Groups**]{style="color: #2C6D26;"}

Data wrangling in R sometimes requires modification or creating a new variable based on certain possible conditions during data analysis.

It is possible to create new variables with the mutate() function in the tidyverse package.

The `case_when()` function allows you to vectorize multiple `if_else` statements, i.e. you test condition-1, and then output output-value-1 if the condition-1 is true, then test condition-2, and output output-value-2 if condition-2 is true, the logical statements continue until you specify a value to output if none of the conditions were true.

However, if we are interested in creating a new variable within a dataframe based on certain conditions with some `if-elif-else` style logic, then the case_when() function is used with the `mutate()` function, both are in the **tidyverse** package.

To illustrate the description in the previous slide, let's consider the image below

![](images/case_when.jpg)

The `TRUE~` is equivalent to the `‘else’` in the `‘if-else’` statement.

It is important to note that the conditions are evaluated in order, therefore one must proceed from the most specific to the most general condition.

Let’s demonstrate how `case_when` works with these examples using the Australia Soybean 1970 dataset.

We are going to create four lodging categories such as \>=1 = 'No Lodging', \>=1.5 = 'Mild Lodging', \>=2.5 = 'Moderate Lodging', and \>=3.5 = 'Heavy Lodging'.

```{r echo=TRUE}
dat <- read_csv("australia.soybean1970.csv")
dat
```

<br><br>

```{r echo=TRUE, tidy=FALSE}
dat_new <- dat |>
     		mutate(
        			lodging_grp = case_when(
                                      lodging >= 3.5 ~ "Heavy Lodging",
                                		  lodging >= 2.5 ~ "Moderate Lodging",
                               		  	lodging >= 1.5 ~ "Mild Lodging",
                                		  lodging >= 1 ~ "No Lodging",
                                		 		       TRUE ~ "NA")
 			 )

```

```{r echo=TRUE, results="tiny"}
dat_new

```

Also, we may be interested in creating a new variable from the existing categorical variable.

Let’s use the new data set `dat_new` to demonstrate this process using `mutate` and `case_when` functions and introduce the `IN` operator `%in%` i.e. We create two lodging categories such that `'No Lodging'` and `'Mild Lodging'` will be `"No Lodging"`, and `'Moderate Lodging'` and `'Heavy Lodging'` will be `"Lodging"`.

```{r echo=TRUE, tidy=FALSE}
dat_new <- dat_new |>
          mutate(
          lodging_grp2 = case_when(
          lodging_grp %in% c("Heavy Lodging","Moderate Lodging")  ~ "Lodging",
          lodging_grp %in% c("No Lodging", "Mild Lodging") ~ "No Lodging",
                                                    TRUE   ~ "NA")
  		)
```

```{r echo=TRUE, results="tiny"}
dat_new

```

<br><br>

# [**Strings**]{style="color: #2C6D26;"}

As we all know that data comes in different formats, numeric or non-numeric (text/string) data format. Data wrangling of non-numeric data sometimes requires splitting of the cell into multiple individual cells or combining multiple cells into a single cell before analysis.

There are some functions in the tidyverse package used for this purpose such as `unite()`, `separate()`, and `str_sub()`.

-   `unite()`: collapse cells across several columns into a single column.

-   `separate()`: separate each cell in a column into several columns.

-   `str_sub()`: extract a substring from a character vector.

Let’s use a fictitious data set called **BadData** to demonstrate how these functions work.

The data set has seven (7) variables: Gen: A combination of location, genotype, and replication, Year: Year of trials, Traits: Measured traits (Trait1 – Trait5)

```{r echo=TRUE, results="tiny"}
dat <- read_csv("BadData.csv")
dat
```

The first task will be to create separate variables for location, genotype, and replication from the Gen variable.

```{r echo=TRUE, tidy=FALSE}
dat1 <- dat |> 
  separate(Gen, sep = "-", into = c("LocGen", "Rep")) |> 
  mutate(
          Location = str_sub(LocGen, 1, 2),
          Gen = str_sub(LocGen, 3, 5)
 	 ) |> 
  select(Location, Gen, Rep, Year, Trait1, Trait2, Trait3, Trait4, Trait5)
```

```{r echo=TRUE, results="tiny"}
dat1
```

Also, we may be interested in creating a new variable which will be a combination of at least two variables. Suppose we want to create a variable called environment `(Env)` using the data set above by combining the Location and Year together.

```{r echo=TRUE, results="tiny"}
dat1 <- dat1 |> 
        unite(Location, Year, col = "Env", sep = "-")
dat1
```

## [**Exercise**]{style="color: #2C6D26;"}

1.  Import Example-02.csv to R and save it to an object named example02 Display the example02 object filter the data by considering: locations Nambour and RedlandBay genotypes G01, G57, and G58, location Brookstead for the year 1970 location Lawes yield between 2 and 3 inclusive, oil greater than 22

2.  Import **Example-02.csv** to R and save to an object named **example02** Create a new variable called `ENV` by combining the first three letters in `loc` and the last two digit in `year`. Categorize the `yield` values of at least 3 as `"High"` and other values as `"Low"` into a new variable called `yield_grp`. Select `ENV`, `gen`, `yield`, `yield_grp`, `height`, and `oil` and transform the dataset using appropriate variables as `traits`.
